{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c61125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "stats = pd.read_csv(\"features.csv\")\n",
    "del stats[\"Unnamed: 0\"]\n",
    "data = stats[stats['TOTAL_seasons'] >= 3]\n",
    "player_list = data['Player'].unique()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split players into training and test sets\n",
    "train_players_list, test_players_list = train_test_split(player_list, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce12b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or_test(player):\n",
    "    if player in train_players_list:\n",
    "        return 'train'\n",
    "    else:\n",
    "        return 'test'\n",
    "\n",
    "data['train_test'] = data['Player'].apply(train_or_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953b85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data['train_test'] == 'train']\n",
    "test = data[data['train_test'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de42e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    if p > 0.95:\n",
    "        p = 0.95\n",
    "    if p < 0.05:\n",
    "        p = 0.05\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d4446ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['minutes_proportion', 'two_point_percentage', 'two_point_attempts', 'three_point_percentage',\n",
    "              'three_point_attempts', 'free_throw_percentage','free_throw_attempts', 'defensive_rebounds',\n",
    "              'offensive_rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'personal_fouls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f529178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cfeatures = ['Age', 'two_point_percentage', 'two_point_attempts', 'three_point_percentage',\n",
    "              'three_point_attempts', 'free_throw_percentage','free_throw_attempts', 'defensive_rebounds',\n",
    "              'offensive_rebounds', 'assists', 'steals', 'blocks', 'turnovers', 'personal_fouls']\n",
    "\n",
    "plt.figure(figsize=(11,9))\n",
    "plt.title('Feature Correlations')\n",
    "sns.heatmap(data[cfeatures].corr(), annot = True, cmap = 'RdYlBu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# For each feature identify strongly correlated features\n",
    "\n",
    "corrs = data.corr()\n",
    "corr_dict = defaultdict(list)\n",
    "\n",
    "for feature in features:\n",
    "    feat_corrs = corrs[feature]\n",
    "    for feat, corr in zip(feat_corrs.index, feat_corrs.values):\n",
    "        if np.abs(corr) > 0.3 and feat != feature and feat[-2] == 'p':\n",
    "            corr_dict[feature].append((feat, corr))\n",
    "\n",
    "for key in corr_dict.keys():\n",
    "    corr_dict[key] = sorted(corr_dict[key], key=lambda tup: -tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {}\n",
    "for feat in list(corr_dict.keys())[1:]:\n",
    "    feat_dict[feat] = ['two_point_percentage_p1', 'two_point_attempts_p1',\n",
    "       'three_point_percentage_p1', 'three_point_attempts_p1',\n",
    "       'free_throw_percentage_p1', 'free_throw_attempts_p1',\n",
    "       'defensive_rebounds_p1', 'offensive_rebounds_p1', 'assists_p1',\n",
    "       'steals_p1', 'blocks_p1', 'turnovers_p1', 'personal_fouls_p1']\n",
    "for key in feat_dict.keys():\n",
    "    feat_dict[key] = feat_dict[key] + ['Age', 'Age2', 'Age3']\n",
    "targets = list(feat_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ee413",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train['season_index'] >= 1][targets]\n",
    "y_train = train[train['season_index'] >= 1]['two_point_attempts']\n",
    "X_test = test[test['season_index'] >= 1][targets]\n",
    "y_test = test[test['season_index'] >= 1]['two_point_attempts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed849eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_general_model(model, X_data, y_data, sig = False, mae = True, r2 = False):\n",
    "    \n",
    "    preds = model.predict(X_data)\n",
    "    \n",
    "    if sig == True:\n",
    "        preds = sigmoid(preds)\n",
    "        y_data = sigmoid(y_data)\n",
    "    \n",
    "    if mae:\n",
    "        return mean_absolute_error(y_data, preds)\n",
    "    \n",
    "    if r2 == False:\n",
    "        return mean_squared_error(y_data, preds)\n",
    "    \n",
    "    else :\n",
    "        return r2_score(y_data, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a3b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP_model = MLPRegressor(random_state=4, max_iter=500)\n",
    "MLP_model.fit(X_train, y_train)\n",
    "evaluate_general_model(MLP_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24209da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_general_model(MLP_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f027e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "grad_model = GradientBoostingRegressor(n_estimators=75)\n",
    "grad_model.fit(X_train, y_train)\n",
    "evaluate_general_model(grad_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a05f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelContainer:\n",
    "    \n",
    "    '''\n",
    "    Object to function as a container for multiple linear models.\n",
    "    Initialize with a dictionary where keys are targets and values are lists of features.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, feature_dict, data):\n",
    "        \n",
    "        self.model_dict = {}\n",
    "        self.feature_dict = feature_dict\n",
    "        \n",
    "        # initialize and fit models\n",
    "        for target in feature_dict.keys():\n",
    "            features = feature_dict[target]\n",
    "            self.model_dict[target] = GradientBoostingRegressor(n_estimators=75).fit(data[features], data[target])    \n",
    "\n",
    "    def predict(self, targets, data, output_df = False, input_df = True, feature_map = None):\n",
    "        \n",
    "        switch = False\n",
    "        \n",
    "        for target in targets:\n",
    "            # select features for target\n",
    "            features = self.feature_dict[target]\n",
    "            \n",
    "            if input_df:\n",
    "                # select model and input data for prediction\n",
    "                preds = self.model_dict[target].predict(data[features])\n",
    "            else:\n",
    "                # if not using a df use feature map to select data from an array\n",
    "                X = np.hstack(tuple([data[:,feature_map[feature]] for feature in features]))\n",
    "                preds = self.model_dict[target].predict(X.reshape(-1,len(features)))\n",
    "            \n",
    "            if not switch:\n",
    "                predictions = preds.reshape(-1,1)\n",
    "                switch = True\n",
    "                \n",
    "            else:\n",
    "                predictions = np.hstack((predictions, preds.reshape(-1,1)))\n",
    "        \n",
    "        if not output_df:\n",
    "            return predictions\n",
    "        \n",
    "        else:\n",
    "            return pd.DataFrame(predictions, columns = targets, index = data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15642d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train['season_index'] >= 1]\n",
    "X_test = test[test['season_index'] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiModel = MultiModelContainer(feat_dict, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "for key in targets:\n",
    "    \n",
    "    feats = feat_dict[key]\n",
    "    \n",
    "    if key[-10:] == 'percentage':\n",
    "        \n",
    "        train_mae = evaluate_general_model(MultiModel.model_dict[key],\n",
    "                                                X_train[feats], X_train[key], sig = True)\n",
    "        test_mae = evaluate_general_model(MultiModel.model_dict[key],\n",
    "                                                X_test[feats], X_test[key], sig = True)\n",
    "        train_rmse = np.sqrt(evaluate_general_model(MultiModel.model_dict[key], \n",
    "                                                    X_train[feats], X_train[key], sig = True, mae = False))\n",
    "        test_rmse = np.sqrt(evaluate_general_model(MultiModel.model_dict[key], \n",
    "                                           X_test[feats], X_test[key], sig = True, mae = False))\n",
    "        train_r2 = evaluate_general_model(MultiModel.model_dict[key],\n",
    "                                                X_train[feats], X_train[key], sig = True, mae = False, r2 = True)\n",
    "        test_r2 = evaluate_general_model(MultiModel.model_dict[key],\n",
    "                                                X_test[feats], X_test[key], sig = True, mae = False, r2 = True)\n",
    "        stddev = sigmoid(data[key]).std()\n",
    "        \n",
    "        results['train_mae'].append(np.round(train_mae, 3))\n",
    "        results['train_zscore'].append(np.round(train_mae/stddev, 3))\n",
    "        results['train_RMSE'].append(np.round(train_rmse, 3))\n",
    "        results['train_R^2'].append(np.round(train_r2, 3))\n",
    "        results['test_mae'].append(np.round(test_mae, 3))\n",
    "        results['test_zscore'].append(np.round(test_mae/stddev, 3))\n",
    "        results['test_RMSE'].append(np.round(test_rmse, 3))\n",
    "        results['test_R^2'].append(np.round(test_r2, 3))\n",
    "        \n",
    "    else:\n",
    "        train_mae = evaluate_general_model(MultiModel.model_dict[key], X_train[feats], X_train[key])\n",
    "        test_mae = evaluate_general_model(MultiModel.model_dict[key], X_test[feats], X_test[key])\n",
    "        train_rmse = np.sqrt(evaluate_general_model(MultiModel.model_dict[key], X_train[feats], X_train[key], mae = False))\n",
    "        test_rmse = np.sqrt(evaluate_general_model(MultiModel.model_dict[key], X_test[feats], X_test[key], mae = False))\n",
    "        train_r2 = evaluate_general_model(MultiModel.model_dict[key], X_train[feats], X_train[key], mae = False, r2 = True)\n",
    "        test_r2 = evaluate_general_model(MultiModel.model_dict[key], X_test[feats], X_test[key], mae = False, r2 = True)\n",
    "        stddev = data[key].std()\n",
    "        \n",
    "        results['train_mae'].append(np.round(train_mae, 3))\n",
    "        results['train_zscore'].append(np.round(train_mae/stddev, 3))\n",
    "        results['train_RMSE'].append(np.round(train_rmse, 3))\n",
    "        results['train_R^2'].append(np.round(train_r2, 3))\n",
    "        results['test_mae'].append(np.round(test_mae, 3))\n",
    "        results['test_zscore'].append(np.round(test_mae/stddev, 3))\n",
    "        results['test_RMSE'].append(np.round(test_rmse, 3))\n",
    "        results['test_R^2'].append(np.round(test_r2, 3))\n",
    "\n",
    "results = pd.DataFrame(results, index = feat_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05554692",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138811f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sum()[['train_zscore', 'test_zscore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699e165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
